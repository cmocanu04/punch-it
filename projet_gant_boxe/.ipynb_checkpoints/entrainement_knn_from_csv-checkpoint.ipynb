{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2678a6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xe. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xe. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "\n",
    "# Imports pour scikit-learn (KNN)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# Affichage dans Jupyter\n",
    "try:\n",
    "    from IPython.display import display, clear_output\n",
    "except ImportError:\n",
    "    display = print \n",
    "    clear_output = lambda wait=False: os.system('cls' if os.name == 'nt' else 'clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def1435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paramètres de Détection des Coups (identiques à analyse_coups.py) ---\n",
    "SMOOTHING_WINDOW_SIZE_KNN = 15\n",
    "ACCEL_Y_PEAK_HEIGHT_KNN = 1.0 # Ajustez si nécessaire pour bien détecter VOS coups de réf.\n",
    "ACCEL_Y_FIRST_PEAK_PROMINENCE_KNN = 0.4\n",
    "ACCEL_Y_SECOND_PEAK_PROMINENCE_KNN = 0.15\n",
    "ACCEL_Y_PEAK_DISTANCE_KNN = 4\n",
    "MAX_GAP_SECONDS_FOR_PAIRS_KNN = 1.0\n",
    "ROTATION_EXTREMA_MARGIN_SECONDS_KNN = 0.25\n",
    "\n",
    "ENERGY_ACTIVITY_THRESHOLD = 4  # Seuil sur l'enveloppe d'énergie (À AJUSTER EMPIRIQUEMENT)\n",
    "MIN_DURATION_SAMPLES_ENERGY = 5 # Durée minimale d'un coup en nombre d'échantillons (ex: 150ms à 100Hz)\n",
    "MERGE_GAP_SAMPLES_ENERGY = 10    # Gap max (échantillons) pour fusionner deux segments actifs\n",
    "SAMPLING_INTERVAL_SECONDS = 0.01  # Intervalle d'échantillonnage de l'Arduino (10ms = 0.01s)\n",
    "\n",
    "# Colonnes de caractéristiques (features) pour le KNN (pour information)\n",
    "FEATURES_COLUMNS = [\n",
    "    'Min_AccX_Val', 'Max_AccX_Val', 'AccX_Amplitude', 'AccX_Ord(1=Max>Min)',\n",
    "    'Min_AccZ_Val', \n",
    "    'Max_Roll_Val', 'Min_Roll_Val', 'Roll_Ord(1=Max>Min)',\n",
    "    'Max_Pitch_Val', 'Min_Pitch_Val', 'Pitch_Ord(1=Max>Min)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323ff0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../joblib/Cristian/dataset_entrainement_knn_cristian.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10348/4115141074.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Nom du fichier CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnom_du_fichier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../joblib/Cristian/dataset_entrainement_knn_cristian.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf_training_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnom_du_fichier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_training_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1661\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../joblib/Cristian/dataset_entrainement_knn_cristian.csv'"
     ]
    }
   ],
   "source": [
    "#Entrainement du fichier commun\n",
    "import pandas as pd\n",
    "\n",
    "# Nom du fichier CSV \n",
    "nom_du_fichier = '../joblib/Cristian/dataset_entrainement_knn_cristian.csv'\n",
    "df_training_features = pd.read_csv(nom_du_fichier)\n",
    "\n",
    "print(df_training_features.head())\n",
    "print(df_training_features.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16769a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Simulation de la création des données d'entraînement ---\n",
      "Le modèle sera entraîné sur 11 caractéristiques.\n",
      "\n",
      "--- 2. Début de l'entraînement du modèle KNN avec les données labellisées ---\n",
      "\n",
      "[DIAGNOSTIC] Comptage des valeurs manquantes AVANT nettoyage :\n",
      "Min_AccX_Val            0\n",
      "Max_AccX_Val            0\n",
      "AccX_Amplitude          0\n",
      "AccX_Ord(1=Max>Min)     0\n",
      "Min_AccZ_Val            0\n",
      "Max_Roll_Val            0\n",
      "Min_Roll_Val            0\n",
      "Roll_Ord(1=Max>Min)     0\n",
      "Max_Pitch_Val           0\n",
      "Min_Pitch_Val           0\n",
      "Pitch_Ord(1=Max>Min)    0\n",
      "Type_Coup               1\n",
      "dtype: int64\n",
      "\n",
      "[CORRECTION] 1 ligne(s) ont été supprimées car 'Type_Coup' était manquant.\n",
      "Les données sont maintenant prêtes pour l'entraînement.\n",
      "\n",
      "Nombre d'échantillons pour l'entraînement après nettoyage: 113\n",
      "Distribution des classes pour l'entraînement:\n",
      "Type_Coup\n",
      "crochet     42\n",
      "uppercut    40\n",
      "direct      31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "1. Imputer entraîné (pour remplir les NaN des features).\n",
      "2. Scaler entraîné (pour mettre les features à la même échelle).\n",
      "3. Modèle KNN entraîné avec succès.\n",
      "    > Classes apprises par le modèle: ['crochet' 'direct' 'uppercut']\n",
      "\n",
      "--- 3. Sauvegarde du modèle, du scaler et de l'imputer ---\n",
      "Modèle KNN sauvegardé dans 'joblib/Cristian\\knn_model_gant.joblib'\n",
      "Scaler sauvegardé dans 'joblib/Cristian\\scaler_gant.joblib'\n",
      "Imputer sauvegardé dans 'joblib/Cristian\\imputer_gant.joblib'\n",
      "\n",
      "=> Tous les composants ont été sauvegardés avec succès.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # Nécessaire pour créer des valeurs NaN pour l'exemple\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# --- 1. SIMULATION DE L'ÉTAT INITIAL ---\n",
    "# Normalement, df_training_features et FEATURES_COLUMNS sont créés dans des cellules précédentes.\n",
    "# Nous les créons ici pour que le script soit complet et exécutable.\n",
    "\n",
    "print(\"--- 1. Simulation de la création des données d'entraînement ---\")\n",
    "\n",
    "# Définition des colonnes de features (caractéristiques)\n",
    "# MISE À JOUR AVEC LES 11 BONNES CARACTÉRISTIQUES\n",
    "FEATURES_COLUMNS = [\n",
    "    'Min_AccX_Val', 'Max_AccX_Val', 'AccX_Amplitude', 'AccX_Ord(1=Max>Min)',\n",
    "    'Min_AccZ_Val',\n",
    "    'Max_Roll_Val', 'Min_Roll_Val', 'Roll_Ord(1=Max>Min)',\n",
    "    'Max_Pitch_Val', 'Min_Pitch_Val', 'Pitch_Ord(1=Max>Min)'\n",
    "]\n",
    "\n",
    "print(f\"Le modèle sera entraîné sur {len(FEATURES_COLUMNS)} caractéristiques.\")\n",
    "\n",
    "# --- 2. ENTRAÎNEMENT DU MODÈLE (CODE CORRIGÉ) ---\n",
    "\n",
    "# Définition des chemins pour la sauvegarde\n",
    "output_dir = \"joblib/Cristian\"\n",
    "MODEL_PATH_JB = os.path.join(output_dir, \"knn_model_gant.joblib\")\n",
    "SCALER_PATH_JB = os.path.join(output_dir, \"scaler_gant.joblib\")\n",
    "IMPUTER_PATH_JB = os.path.join(output_dir, \"imputer_gant.joblib\")\n",
    "\n",
    "# Créer le dossier de sortie s'il n'existe pas\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Vérification que le DataFrame existe (cette partie est gardée de votre code original)\n",
    "# Pour que ce script fonctionne, vous devez définir 'df_training_features' en amont.\n",
    "# Exemple: df_training_features = pd.read_csv('votre_fichier_entrainement.csv')\n",
    "if 'df_training_features' in locals() and isinstance(df_training_features, pd.DataFrame) and not df_training_features.empty:\n",
    "    print(\"\\n--- 2. Début de l'entraînement du modèle KNN avec les données labellisées ---\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # CORRECTION APPLIQUÉE ICI\n",
    "    # =================================================================\n",
    "    # Diagnostic : On vérifie les valeurs manquantes avant toute action.\n",
    "    print(\"\\n[DIAGNOSTIC] Comptage des valeurs manquantes AVANT nettoyage :\")\n",
    "    print(df_training_features.isnull().sum())\n",
    "    \n",
    "    # Correction : On supprime les lignes où l'étiquette 'Type_Coup' est manquante.\n",
    "    lignes_avant = len(df_training_features)\n",
    "    df_training_features.dropna(subset=['Type_Coup'], inplace=True)\n",
    "    lignes_apres = len(df_training_features)\n",
    "    \n",
    "    if lignes_avant > lignes_apres:\n",
    "        print(f\"\\n[CORRECTION] {lignes_avant - lignes_apres} ligne(s) ont été supprimées car 'Type_Coup' était manquant.\")\n",
    "        print(\"Les données sont maintenant prêtes pour l'entraînement.\\n\")\n",
    "    # =================================================================\n",
    "    \n",
    "    # Séparation des données en features (X) et étiquettes (y)\n",
    "    X_ref = df_training_features[FEATURES_COLUMNS]\n",
    "    y_ref = df_training_features['Type_Coup']\n",
    "\n",
    "    print(f\"Nombre d'échantillons pour l'entraînement après nettoyage: {len(X_ref)}\")\n",
    "    print(f\"Distribution des classes pour l'entraînement:\\n{y_ref.value_counts()}\")\n",
    "\n",
    "    # a. Imputation des valeurs manquantes dans les FEATURES (X_ref)\n",
    "    IMPUTER = SimpleImputer(strategy='median')\n",
    "    X_ref_imputed = IMPUTER.fit_transform(X_ref)\n",
    "    print(\"\\n1. Imputer entraîné (pour remplir les NaN des features).\")\n",
    "\n",
    "    # b. Standardisation des données\n",
    "    SCALER = StandardScaler()\n",
    "    X_ref_scaled = SCALER.fit_transform(X_ref_imputed)\n",
    "    print(\"2. Scaler entraîné (pour mettre les features à la même échelle).\")\n",
    "\n",
    "    # c. Entraînement du classifieur KNN\n",
    "    KNN_MODEL = KNeighborsClassifier(n_neighbors=3)\n",
    "    # Cette ligne ne lèvera plus l'erreur \"Input contains NaN\"\n",
    "    KNN_MODEL.fit(X_ref_scaled, y_ref) \n",
    "    print(\"3. Modèle KNN entraîné avec succès.\")\n",
    "    print(f\"    > Classes apprises par le modèle: {KNN_MODEL.classes_}\")\n",
    "\n",
    "    # --- 3. SAUVEGARDE DES COMPOSANTS ENTRAÎNÉS ---\n",
    "    print(\"\\n--- 3. Sauvegarde du modèle, du scaler et de l'imputer ---\")\n",
    "    try:\n",
    "        joblib.dump(KNN_MODEL, MODEL_PATH_JB)\n",
    "        print(f\"Modèle KNN sauvegardé dans '{MODEL_PATH_JB}'\")\n",
    "        \n",
    "        joblib.dump(SCALER, SCALER_PATH_JB)\n",
    "        print(f\"Scaler sauvegardé dans '{SCALER_PATH_JB}'\")\n",
    "        \n",
    "        joblib.dump(IMPUTER, IMPUTER_PATH_JB)\n",
    "        print(f\"Imputer sauvegardé dans '{IMPUTER_PATH_JB}'\")\n",
    "        \n",
    "        print(\"\\n=> Tous les composants ont été sauvegardés avec succès.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERREUR lors de la sauvegarde d'un composant: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nLe DataFrame 'df_training_features' n'est pas disponible ou est vide.\")\n",
    "    print(\"Veuillez charger vos données dans un DataFrame nommé 'df_training_features' avant d'exécuter cette cellule.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbf319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
